---
slug: HW3
title: "STudy Group 7: Homework 3"
author: "Study Group 7: Thirat Wongwaisayawan, Lejla Kajevic, Francesco Nicolì, Zitong Hu, Rufei Wang, Siddharth Gulati"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(GGally)
library(readxl)
library(here)
library(skimr)
library(janitor)
library(broom)
library(tidyquant)
library(infer)
library(openintro)
```


# Youth Risk Behavior Surveillance

Every two years, the Centers for Disease Control and Prevention conduct the [Youth Risk Behavior Surveillance System (YRBSS)](https://www.cdc.gov/healthyyouth/data/yrbs/index.htm) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.

## Load the data

This data is part of the `openintro` textbook and we can load and inspect it. There are observations on 13 different variables, some categorical and some numerical. The meaning of each variable can be found by bringing up the help file:

?yrbss

```{r}
data(yrbss)
glimpse(yrbss)
```

Before you carry on with your analysis, it's is always a good idea to check with `skimr::skim()` to get a feel for missing values, summary statistics of numerical variables, and a very rough histogram.

## Exploratory Data Analysis


```{r, eda_on_weight, fig.height=5, fig.width=8}

 #Summarising the variable
yrbss %>%
  skim(weight)

#Plotting the variable
ggplot(yrbss, aes(x=weight, na.rm=TRUE)) + geom_density()

```

According to the summary statistics, there are 1004 missing weights data in the data set. We can also visualise the distribution of the variable "weight" and see how it is a heavily right-skewed distribution, with a most recurrent value of about 60 kg.

```{r, fig.height=5, fig.width=8}
correlation_summary <- yrbss %>% 
  group_by(physically_active_7d) %>% 
  summarise(average_weight = mean(weight, na.rm = TRUE))

ggplot(correlation_summary, aes(x=physically_active_7d, y=average_weight)) + geom_col()
ggplot(yrbss, aes(x=weight)) + geom_density() + facet_wrap(~physically_active_7d)

#Building physical activity variable
yrbss_1 <- yrbss %>% 
  mutate(physical_3plus=ifelse(physically_active_7d>=3,"yes","no"))

yrbss_1
#Calculating the percentage using count()
yrbss_1 %>%
  count(physical_3plus, sort=TRUE) %>%
  mutate(Percentage=n/sum(n))

#Calculating the percentage using group_by() and summarise()
 yrbss_1 %>% 
  group_by(physical_3plus) %>% 
  summarise(count=n()) %>% 
  mutate(Percentage=count/sum(count))

```

We included the "Not Available" values since those are still part of the sample but we are just not sure about the "physical activity" of those kids. Hence, excluding them would have led to deceptive proportions for both the kids exercising at least three times a week and those who exercise less.
  
```{r, mutate_and_count}

Proportion_formula_ci <- yrbss_1 %>% 
  select(physical_3plus) %>%
  count(physical_3plus) %>%
  summarise(Proportion=n/sum(n),
            t_critical=qt(0.975,n-1),
            se_proportion=sqrt((Proportion*(1-Proportion)/n)),
            margin_of_error=t_critical*se_proportion,
            Lower_limit=Proportion-margin_of_error,
            Upper_limit=Proportion+margin_of_error)

Proportion_formula_ci

```
Can you provide a 95% confidence interval for the population proportion of high schools that are *NOT* active 3 or more days per week?

  
Make a boxplot of `physical_3plus` vs. `weight`. Is there a relationship between these two variables? What did you expect and why?

```{r, boxplot}
Mutated_yrbss <- yrbss_1 %>% 
  filter(physical_3plus == "yes"|physical_3plus=="no")

#Build the plot
ggplot(Mutated_yrbss, aes(y=weight)) + geom_boxplot() + facet_wrap(~physical_3plus)

```

At a younger age, it is logical to think that more physical exercise would lead to a lower weight, since it is unlikely that a kid gains weight and muscles through physical activity. Hence, the predominant effect of exercising would a decreasing fat and, consequently, a lower weight. Given the two boxplots, though, we can see how the median weight for the kids exercising three times a week or more is actually higher than the one for those who exercise less. The two distributions are still both right-skewed but the skeweness is more pronounced for those who exercise less. This results in a higher median, even though the difference is minimal.

## Confidence Interval

Boxplots show how the medians of the two distributions compare, but we can also compare the means of the distributions using either a confidence interval or a hypothesis test. Note that when we calculate the mean, SD, etc. weight in these groups using the mean function, we must ignore any missing values by setting the `na.rm = TRUE`.


```{r, ci_using_formulas}
Mutated_yrbss <- yrbss_1 %>% 
  group_by(physical_3plus) %>% 
  summarise(Average_weight=mean(weight, na.rm=TRUE),
            Standard_Deviation=sd(weight, na.rm=TRUE),
            Sample_size=n(),
            Standard_error=(Standard_Deviation/sqrt(Sample_size)),
            t_critical=qt(0.975,Sample_size-1),
            Lower_limit=(Average_weight-t_critical*Standard_error),
            Upper_limit=(Average_weight+t_critical*Standard_error)) %>% 
  filter(!is.na(physical_3plus))

Mutated_yrbss
```

There is an observed difference of about 1.77kg (68.44 - 66.67), and we notice that the two confidence intervals do not overlap. It seems that the difference is at least 95% statistically significant. Let us also conduct a hypothesis test.

## Hypothesis test with formula

Write the null and alternative hypotheses for testing whether mean weights are different for those who exercise at least times a week and those who don’t.

```{r, t_test_using_R}

t.test(weight ~ physical_3plus, data = yrbss_1)
```
Given that the Greek letter "delta" represents the difference between the two mean weights, we can represent the two hypotheses for the statistical test in this way:

$H_{0}: \delta = 0\\$$H_{1}: \delta \not= 0$ 

## Hypothesis test with `infer`


Next, we will introduce a new function, `hypothesize`, that falls into the infer workflow. You will use this method for conducting hypothesis tests.

But first, we need to initialize the test, which we will save as `obs_diff`.

```{r, calc_obs_difference}
obs_diff <- yrbss_1 %>%
  filter(!is.na(physical_3plus)) %>% 
  specify(weight ~ physical_3plus) %>%
  calculate(stat = "diff in means", order = c("yes", "no"))

obs_diff
```



Notice how you can use the functions specify and calculate again like you did for calculating confidence intervals. Here, though, the statistic you are searching for is the difference in means, with the order being yes - no != 0.

After you have initialized the test, you need to simulate the test on the null distribution, which we will save as null.


```{r, hypothesis_testing_using_infer_package}

null_dist <- yrbss_1 %>%
  filter(!is.na(physical_3plus)) %>%
  # specify variables
  specify(weight ~ physical_3plus) %>%
  
  # assume independence, i.e, there is no difference
  hypothesize(null = "independence") %>%
  
  # generate 1000 reps, of type "permute"
  generate(reps = 1000, type = "permute") %>%
  
  # calculate statistic of difference, namely "diff in means"
  calculate(stat = "diff in means", order = c("yes", "no"))

null_dist
```


Here, `hypothesize` is used to set the null hypothesis as a test for independence, i.e., that there is no difference between the two population means. In one sample cases, the null argument can be set to *point* to test a hypothesis relative to a point estimate.

Also, note that the `type` argument within generate is set to permute, which is the argument when generating a null distribution for a hypothesis test.

We can visualize this null distribution with the following code:

```{r, fig.height=5, fig.width=8}
ggplot(data = null_dist, aes(x = stat)) +
  geom_histogram()

```


Now that the test is initialized and the null distribution formed, we can visualise to see how many of these null permutations have a difference of at least `obs_stat` of `r obs_diff %>% pull() %>% round(2)`?

We can also calculate the p-value for your hypothesis test using the function `infer::get_p_value()`.

```{r, fig.height=5, fig.width=8}

null_dist %>% visualize() +
  shade_p_value(obs_stat = obs_diff, direction = "two-sided")

null_dist %>%
  get_p_value(obs_stat = obs_diff, direction = "two_sided")

```


This the standard workflow for performing hypothesis tests.

# IMDB ratings: Differences between directors

Recall the IMBD ratings data. I would like you to explore whether the mean IMDB rating for Steven Spielberg and Tim Burton are the same or not. I have already calculated the confidence intervals for the mean ratings of these two directors and as you can see they overlap. 

Null Hypothesis: IMDB mean rating is the same for SS and TB. difference in m =0
Alternative hypothesis: difference in mean is not equal to 0.
t-stat: difference between mean divided by standard error
p-value


```{r load-movies-data,fig.height=5, fig.width=8 }
movies <- read_csv(here::here("data", "movies.csv"))
#glimpse(movies)

movies_2 <- movies %>% 
  group_by(director) %>% 
  filter(director %in% c("Steven Spielberg","Tim Burton")) %>% 
  summarise(rating,mean_rating = mean(rating), #find datapoints needed for t distribution.
         sd_rating = sd(rating),
         count=n(),
         t_critical =qt(0.975,count-1),
         se_rating = sd(rating)/sqrt(count),
         margin_of_error = t_critical*se_rating, 
         rating_low = mean_rating - margin_of_error, 
         rating_high = mean_rating +margin_of_error,
         highest_rating =max(rating),
         lowest_rating = min(rating))

#movies_mean<-count(movies, mean_rating) 
#movies_mean
movies_2

  ggplot(movies_2, aes(x = mean_rating,y = director))+
  geom_point() +
  geom_errorbar(data = movies_2,aes(xmin = rating_low, xmax=rating_high,height=0.2), size = 1, height = 0.1)+
  labs(x = "Mean IMDB Rating",title = "Do Spielburg and Burton have the same mean IMDB rating",subtitle="95% confidence interval overlap")+
    geom_rect(aes(xmin=7.27, xmax= 7.33, ymin=0, ymax=3),
            fill = "grey70",
            alpha = 0.5)+
    theme_minimal()#+
   # geom_text(aes(x= mean_rating, label = labels), vjust=0, nudge_y =0.05,overlap=FALSE)
  #geom_text(label=mean)
  #theme_bw()

```

## Analysis

```{r, fig.height=5, fig.width=8}
t.test(rating~director, data = movies_2) #find t-stat, p-value

#simulating a null world using infer()
set.seed(1234)
ratings_in_null_world <- movies_2 %>%
  specify(rating~director) %>% # we want to look at ratings of directors
  hypothesize(null = "independence") %>%  # hypothesize that the difference is 0
  generate(reps=1000, type="permute") %>% # create a bunch of simulated samples
  calculate(stat="diff in means", order = c("Steven Spielberg","Tim Burton")) # find difference in means of each sample. 

#ratings_in_null_world visualize, with shaded p value
ratings_in_null_world %>% visualize() +
  shade_p_value(obs_stat = 0.64, direction = "two-sided")

 # finding p-value of simulated distribution
p_value <- ratings_in_null_world%>%
  get_p_value(obs_stat = .64, direction = "two-sided")
p_value

```
With a p-value of 1%<5%, we can reject the null hypothesis that Spielburg and Burton have the same mean rating.  
You should use both the `t.test` command and the `infer` package to simulate from a null distribution, where you assume zero difference between the two.

By using the infer package, we can simulate from a null distribution and compute p-values (with get_p_value()). As seen from above, the p-value from the null distribution gives 0.4%<1%<5%. Thus, we can  infer that the null hypothesis can be rejected.

We can conclude that the difference in mean ratings for the two directors is unlikely to be zero. 

# Omega Group plc- Pay Discrimination


At the last board meeting of Omega Group Plc., the headquarters of a large multinational company, the issue was raised that women were being discriminated in the company, in the sense that the salaries were not the same for male and female executives. A quick analysis of a sample of 50 employees (of which 24 men and 26 women) revealed that the average salary for men was about 8,700 higher than for women. This seemed like a considerable difference, so it was decided that a further analysis of the company salaries was warranted. 

You are asked to carry out the analysis. The objective is to find out whether there is indeed a significant difference between the salaries of men and women, and whether the difference is due to discrimination or whether it is based on another, possibly valid, determining factor. 

## Loading the data


```{r load_omega_data}
omega <- read_csv(here::here("data", "omega.csv"))
glimpse(omega) # examine the data frame
```

## Relationship Salary - Gender ?

Calculate summary statistics on salary by gender. Also, create and print a dataframe where, for each gender, you show the mean, SD, sample size, the t-critical, the SE, the margin of error, and the low/high endpoints of a 95% condifence interval

```{r, confint_single_valiables, }
# Summary Statistics of salary by gender
omega_2<-mosaic::favstats (salary ~ gender, data=omega)
omega_2

# Dataframe with two rows (male-female) and having as columns gender, mean, SD, sample size,
male_female_df <- omega_2 %>%
  select(mean, sd,n, gender) %>%
  mutate(se=sd/sqrt(n),
  t = qt(0.975,n-1),
  margin =t*se,
  l_ci =mean - margin,
  h_ci = mean +margin) %>%
  select(gender, mean, sd, n, t, se, margin, l_ci, h_ci)

male_female_df


# the t-critical value, the standard error, the margin of error, 
# and the low/high endpoints of a 95% confidence interval
# The 95% confidence intervals of the two genders overlap largely, meaning that there is now a need to run t-tests to determine whether these two means are different.

```


You can also run a hypothesis testing, assuming as a null hypothesis that the mean difference in salaries is zero, or that, on average, men and women make the same amount of money. You should tun your hypothesis testing using `t.test()` and with the simulation method from the `infer` package.

```{r, hypothesis_testing, fig.height=5, fig.width=8}
# hypothesis testing using t.test() 

t.test(salary~gender, data = omega) #find t-stat, p-value

mean_dif <- omega %>% specify(salary~gender) %>% calculate(stat = "diff in means", order = c("male","female"))

#simulating a null world using infer()
set.seed(1234)
ratings_in_null_world <- omega %>%
  specify(salary~gender) %>% # we want to look at ratings of directors
  hypothesize(null = "independence") %>%  # hypothesize that the difference is 0
  generate(reps=1000, type="permute") %>% # create a bunch of simulated samples
  calculate(stat="diff in means", order = c("male","female")) # find difference in means of each sample. 

#ratings_in_null_world visualize, with shaded p value
ratings_in_null_world %>% visualize() +
  shade_p_value(obs_stat = mean_dif , direction = "two-sided")

 # finding p-value of simulated distribution
p_value <- ratings_in_null_world%>%
  get_p_value(obs_stat = mean_dif, direction = "two-sided")
p_value

# hypothesis testing using infer package


```
From running the simulation, we saw that the p-value is close to 0 (2e-04). This indicates that we should reject the null hypothesis and the difference in gender salary is indeed statistically different.

- We can also get the same conclusion from looking at the t-statistic. In this case, the absolute value of t-statistic is higher than 2, thus we reject the null hypothesis.

- The third way to confirm our conclusion is to look at the confidence interval. In this case, the CI does not contain zero, thus we reject the null hypothesis.


## Relationship Experience - Gender?

At the board meeting, someone raised the issue that there was indeed a substantial difference between male and female salaries, but that this was attributable to other reasons such as differences in experience. A questionnaire send out to the 50 executives in the sample reveals that the average experience of the men is approximately 21 years, whereas the women only have about 7 years experience on average (see table below).

```{r, experience_stats, fig.height=5, fig.width=8}
# Summary Statistics of salary by gender
favstats (experience ~ gender, data=omega)

t.test(experience~gender, data = omega) #find t-stat, p-value

mean_dif <- omega %>% specify(experience~gender) %>% calculate(stat = "diff in means", order = c("male","female"))

#simulating a null world using infer()
set.seed(1234)
ratings_in_null_world <- omega %>%
  specify(experience~gender) %>% # we want to look at ratings of directors
  hypothesize(null = "independence") %>%  # hypothesize that the difference is 0
  generate(reps=1000, type="permute") %>% # create a bunch of simulated samples
  calculate(stat="diff in means", order = c("male","female")) # find difference in means of each sample. 

#ratings_in_null_world visualize, with shaded p value
ratings_in_null_world %>% visualize() +
  shade_p_value(obs_stat = mean_dif , direction = "two-sided")

 # finding p-value of simulated distribution
p_value <- ratings_in_null_world%>%
  get_p_value(obs_stat = mean_dif, direction = "two-sided")
p_value

```

Based on this evidence, can you conclude that there is a significant difference between the experience of the male and female executives? Perform similar analyses as in the previous section. Does your conclusion validate or endanger your conclusion about the difference in male and female salaries?  

*Again, there are three ways to tell whether there is a statistically significant difference in the experience of male and female executives. 

+ Firstly, the confidence interval does not contain zero, thus we reject the null hypothesis and there is a significant difference in experience of the male and female executives.
+ t-statistic is larger than 2, thus we reject the null-hypothesis.
+ p-value is smaller than 5%, thus we reject the null-hypothesis.

Our conclusion endangers the conclusion about the difference in male and female salaries. This is because the difference in experience is indicating that there are other factors than gender itself that can explain the difference in salary. The conclusion that there is gender discrimination in salary may be violated. Thus, further investigation is required to confirm whether other factors can also affect the difference in salaries.

## Relationship Salary - Experience ?

Someone at the meeting argues that clearly, a more thorough analysis of the relationship between salary and experience is required before any conclusion can be drawn about whether there is any gender-based salary discrimination in the company.

```{r, salary_exp_scatter, fig.height=5, fig.width=8}

ggplot(omega, aes(salary, experience)) + geom_point()
```


## Check correlations between the data


```{r, ggpairs, fig.height=5, fig.width=8}
omega %>% 
  select(gender, experience, salary) %>% #order variables they will appear in ggpairs()
  ggpairs(aes(colour=gender, alpha = 0.3))+
  theme_bw()
```


Salary vs Experience scatterplot: We can see that there is a positive correlation between experience and salary for both genders. The more years of work experience, the higher the salaries. The correlation coefficients (approximately 0.803 for both, 0.812 for females and 0.661 for males) calculated above confirm the observations we drew from looking at the scatterplot.

From the boxplots above we can also easily tell that the median work experience is much less for female executives and that female median salary is also smaller, but to a lesser extent. From the density graphs, we can see that the distribution of work experience for female is concentrated on the lower ends. 

The above observations are suggesting that the 'number of years worked' is very likely to be a valid underlying factor to explain the gender salary gap. However, we cannot totally eliminate gender discrimination as a factor for gender pay gap. For instance, it is worth looking into whether there are gender related issues that prevent female executives to attain more work experience in general? 

It would also be interesting to look at the age differences in female and male executives because that could be the underlying reason why there are significant differences in their average work experience.

# Challenge 1: Brexit plot

```{r}
#load brexit data
brexit <- read_csv(here::here("data", "brexit_results.csv"))
skim(brexit)
```


```{r brexit_challenge, fig.height=5, fig.width=8}
#knitr::include_graphics(here::here("images", "brexit.png"), error = FALSE)
#transferring the data into long format instead of wide
brexit <- brexit %>% rename(Conservative=con_2015, Labour = lab_2015, Lib_Dems = ld_2015, UKIP = ukip_2015)

brexit2<- brexit %>% pivot_longer(cols=c('Conservative', 'Labour','Lib_Dems', 'UKIP'),
                    names_to='parties',
                    values_to='parties_share')
#plot the data
ggplot(brexit2, aes(x =parties_share, y=leave_share, color = parties))+geom_point()+geom_smooth(method=lm)+ 
  scale_color_manual(breaks = c('Labour','Conservative', 'Lib_Dems', 'UKIP'),
                        values=c("#aa1619", "#3284bf","#ebbf58","#e2e039"))+
  labs(x = "Party % in the UK 2015 general election")+
  labs( y = "Leave % in the 2016 Brexit Referendum")+
  ggtitle("How polital affiliation translated to Brexit voting")
```


# Challenge 2:GDP components over time and among countries

At the risk of oversimplifying things, the main components of gross domestic product, GDP are personal consumption (C), business investment (I), government spending (G) and net exports (exports - imports). You can read more about GDP and the different approaches in calculating at the [Wikipedia GDP page](https://en.wikipedia.org/wiki/Gross_domestic_product).

The GDP data we will look at is from the [United Nations' National Accounts Main Aggregates Database](https://unstats.un.org/unsd/snaama/Downloads), which contains estimates of total GDP and its components for all countries from 1970 to today. We will look at how GDP and its components have changed over time, and compare different countries and how much each component contributes to that country's GDP. The file we will work with is [GDP and its breakdown at constant 2010 prices in US Dollars](http://unstats.un.org/unsd/amaapi/api/file/6) and it has already been saved in the Data directory. Have a look at the Excel file to see how it is structured and organised


```{r read_GDP_data}

UN_GDP_data  <-  read_excel(here::here("data", "Download-GDPconstant-USD-countries.xls"), # Excel filename
                sheet="Download-GDPconstant-USD-countr", # Sheet name
                skip=2) # Number of rows to skip

  
```


```{r reshape_GDP_data}

tidy_GDP_data  <-  UN_GDP_data%>% 
  pivot_longer(cols = 4:51, names_to='years',values_to='components_GDP') %>% 
  mutate(components_GDP_billions =components_GDP/(1000000000) )

glimpse(tidy_GDP_data)


# Let us compare GDP components for these 3 countries
country_list <- c("United States","India", "Germany")
```
## GDP components over time


```{r gdp1,  out.height="100%"}

GDPdata2 <- tidy_GDP_data
old_name<- c("Gross capital formation",
             "Exports of goods and services",
             "General government final consumption expenditure", 
             "Household consumption expenditure (including Non-profit institutions serving households)", 
             "Imports of goods and services")


new_name<- c( "Gross capital formation",
              "Exports",
              "Government expenditure",
              "Household expenditure",
              "Imports")


names(new_name) <-old_name
GDPdata2$IndicatorName <- new_name[GDPdata2$IndicatorName]
GDPdata2 <- GDPdata2 %>%
  na.omit(IndicatorName)
```


```{r gdp_drawing,fig.height=5, fig.width=8}
GDP_components_over_time <- GDPdata2 %>% filter(Country %in% country_list) %>% group_by(IndicatorName)
#GDP_components_over_time <- GDP_components_over_time %>% filter(IndicatorName == 'Household expenditure')

ggplot(GDP_components_over_time, 
       aes(x=years,y=components_GDP_billions,colour = IndicatorName))+
  geom_line(aes(group=IndicatorName))+
  facet_wrap(~Country) +
  labs(title="GDP components over time", subtitle="In constant 2010 USD") +
  theme(axis.text=element_text(size=3))+
  scale_x_discrete(breaks = c(1970,1980,1990,2000,2010,2020))+
  theme_bw()
#+theme_minimal()
#+xlim(1970,2020)
```
## GDP and its breakdown at constant 2010 prices in US dollars

> What is the % difference between what you calculated as GDP and the GDP figure included in the dataframe?
The % difference between the GDP figure we calculated and that in the dataframe is 0.867%.

```{r gdp2, fig.height=5, fig.width=8}
GDP_components_over_time2 <- GDP_components_over_time %>%
  select(-components_GDP) %>% 
  pivot_wider(names_from=IndicatorName,
              values_from = components_GDP_billions)

colnames(GDP_components_over_time2) <-c("CountryID","Country","years","Household_expenditure","Government_expenditure","Gross_capital_formation","Exports","Imports")

GDP_components_over_time3 <- GDP_components_over_time2 %>% 
  mutate(Net_exports=Exports-Imports,GDP_total=(Household_expenditure+Government_expenditure+Gross_capital_formation+Exports-Imports)) %>%
  select(-Exports,-Imports) %>%
  pivot_longer(4:7,names_to ="IndicatorName",values_to="GDP_components_billions") %>%
  mutate(Relative_value=GDP_components_billions/GDP_total)

ggplot(GDP_components_over_time3, 
       aes(x=years,y=Relative_value, colour=IndicatorName, group=IndicatorName)) + 
  geom_line() + 
  facet_wrap(~Country) + 
  scale_x_discrete(breaks = c(1970,1980,1990,2000,2010,2020)) + 
  scale_y_continuous(labels=scales::percent) + 
  theme_bw() + 
  labs(title = "GDP and its breakdown at constant 2010 prices in US Dollars", x="years")
```



> What is this last chart telling you? Can you explain in a couple of paragraphs the different dynamic among these three countries? 

The last chart shows us a clear breakdown of GDP from 1970 to 2020 spread across 4 key categories - Government Expenses, Gross Capital Formation, House Expenses and Net Exports. 

In case of House Expenditure, Germany sees a fairly constant trend over the years while the trend is extremely volatile in case of India and US. India sees a ~15pp drop in theb category while US sees a ~10pp rise. A very similar, but inverted trend is seen in case of the Gross Capital Formation. 

Germany sees a minor drop over the years in this category while India sees a major jump, almost 20pp from 1970 to 2010 after which it drops a bit. The United States trend for the Gross Capital Formation is fairly flat at ~20% with very little variation over the years. The lowest two categories - Government Expenditure and Net Exports see mostly flat trends too. The net exports are almost 0% in all three regions - India, Germany and United States. The line for Government Expenditure is constant for Germany and India, while it sees a declining trend in case of United States.


# Details

- Who did you collaborate with:  Lejla Kajevic, Francesco Nicolì, Zitong Hu, Rufei Wang, Siddharth Gulati
