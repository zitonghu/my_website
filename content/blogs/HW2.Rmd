---

title: "Session 4: Homework 2"
author: "Study Group 7, Thirat Wongwaisayawan, Lejla Kajevic, Francesco Nicol√¨, Zitong Hu, Rufei Wang, Siddharth Gulati"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
editor_options: 
  markdown: 
    wrap: 72
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(dplyr)
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(wbstats)
library(countrycode)
library(patchwork)
library(gganimate)
library(infer)
```

# Climate change and temperature anomalies

We can find data on the *Combined Land-Surface Air and Sea-Surface Water Temperature Anomalies* in the
Northern Hemisphere at [NASA's Goddard Institute for Space
Studies](https://data.giss.nasa.gov/gistemp). The [tabular data of
temperature anomalies can be found
here](https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.txt)



```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```




```{r tidyweather}
#inspecting data.frame 'weather'

#glimpse(weather) 

# selecting the year and 12 month variables
weather_clean <- weather %>% 
  select(1:13) 

# converting to the dataframe to longer format (tidyweather)
tidyweather <- pivot_longer(weather_clean, cols = 2:13, names_to = "Month", values_to = "delta")

tidyweather
```

## Plotting Information

We plot the data using a time-series scatter plot, and add a
trendline. To do that, we first need to create a new variable called
`date` in order to ensure that the `delta` values are plot
chronologically.


```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), Month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies"
  )

```

## Is the effect of increasing temperature more pronounced in some months?
Use `facet_wrap()` to produce a seperate scatter plot for each month,
again with a smoothing line.




```{r facet_wrap}
ggplot(tidyweather) +
  aes(x = date, y = delta) +
  geom_point () +
  geom_smooth (color = "red") +
  theme_bw() +
  labs (title = "Weather Changes by Month") +
  facet_wrap(~ month)
```
-   Delta on the y-axis, represents the temperature deviations from the
    expected value. The red smooth lines we drew are the trend lines for
    the temperature changes. The steeper the smooth lines are, the
    higher the rates of temperature increases.
    
-   Before 1950, visually, we can see that for Januarys and Februaries,
    the effect of increasing temperature was more pronounced. For
    instance, the smooth line drawn for January is much steeper than
    that of July and August.
    
-   We can tell that the year 1950 was roughly a point, after which, the
    smooth line becomes much steeper for all months compared to
    before 1950. There are months where the effect of increasing
    temperature was more pronounced, but these effects are not
    distinctive visually.


## Removed Data before 1880
It is sometimes useful to group data into different time periods to
study historical data. For example, we often refer to decades such as
1970s, 1980s, 1990s etc. to refer to a period of time. NASA calcuialtes
a temperature anomaly, as difference form the base periof of 1951-1980.
The code below creates a new data frame called `comparison` that groups
data in five time periods: 1881-1920, 1921-1950, 1951-1980, 1981-2010
and 2011-present.

We remove data before 1800 and before using `filter`. Then, we use the
`mutate` function to create a new variable `interval` which contains
information on which period each observation belongs to. We can assign
the different periods using `case_when()`.

```{r intervals}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```


Now that we have the `interval` variable, we can create a density plot
to study the distribution of monthly deviations (`delta`), grouped by
the different time periods we are interested in. Set `fill` to
`interval` to group and colour the data by different time periods.

```{r density_plot}
ggplot(comparison) +
  aes(x = delta, fill = interval) +
  geom_density(alpha = 0.3) +
  theme_minimal(base_size = 18)


```

So far, we have been working with monthly anomalies. However, we might
be interested in average annual anomalies. We can do this by using
`group_by()` and `summarise()`, followed by a scatter plot to display
the result.

```{r averaging}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
# creating summaries for mean delta 
# use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(mean_delta = mean(delta), na.rm = TRUE) 

#plotting the data:
ggplot(average_annual_anomaly) +
  aes(x = Year, y = mean_delta) +
  geom_point() +
  
  
#Fit the best fit line, using LOESS method
#change theme to theme_bw() to have white background + black frame around plot
  aes(x = Year, y = mean_delta) +
  geom_smooth(method = loess, color = "red") +
  theme_bw( base_size = 18) +
  labs (title = "Average Annual Anomolies Over Time", x = "Year", y = "Average Annual Temperature Change") 

```

## Confidence Interval for `delta`
### Using the formula

```{r, calculate_CI_using_formula}

# choose the interval 2011-present
# what dplyr verb will you use? 
  formula_ci <- comparison %>% 
  filter(interval == "2011-present") %>% 
  
# calculate summary statistics for temperature deviation (delta) 
# calculate mean, SD, count, SE, lower/upper 95% CI
# what dplyr verb will you use
    summarise(mean = mean(delta, na.rm = TRUE),
            count = n(),
            SD = sd(delta, na.rm = TRUE),
            SE = SD / sqrt(count),
            # find t critical value with n-1 degrees of freedom
            t_critical = qt(0.975,count -1),
            margin_of_error = SE * t_critical,
            lower_95pct_CI = mean - margin_of_error,
            upper_95pct_CI = mean + margin_of_error)
  
# print out formula_CI
formula_ci
```


### Using Infer() package
```{r CI_infer}
# calculate ci using the infer package (bootstrap simulation)
set.seed(1234)
boot_ci <- comparison %>% 
  # select the interval of interest
  filter(interval == "2011-present") %>% 
  # specify the variable of interest
  specify (response = delta) %>% 
  # generate bootstrap samples 
  generate (reps = 1000, type = "bootstrap") %>% 
  # find mean
  calculate(stat = "mean") %>% 
  # find the ci
  get_confidence_interval(level = 0.95, type = "percentile")

# print out boot_ci
boot_ci
```
* We are 95% confident that the average temperature change in the 2011-present time period is between 1.02 and 1.11 degree-celcius.
* We get very similar results for confidence intervals from the bootstrap method as well as the formula method.



# Biden's Approval Margins


```{r Biden, cache=TRUE}
approval_polllist <- read.csv('https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv') 

approval_polllist <- approval_polllist %>% 
  mutate(enddate = mdy(enddate), 
         year = year(enddate),
         month = month(enddate), #default format in r is year ymd
         week = isoweek(enddate)) # using lubridate to change the format of the dates. 

approval_polllist<- approval_polllist%>%
  filter(year == 2022) %>% # filter to remove all other years other than 2022.
  group_by(week, subgroup) %>% # grouping by week then subgroup, so that we can conitnue analysis
  summarise(app_disapp = approve-disapprove, #finding the net ratings
          mean_approve_disapprove = mean(app_disapp), # finding mean of net ratings
          sd_rating = sd(app_disapp), # finding sd
          count=n(), # counting number of polls in each week
          t_critical=qt(0.975,count-1) , #t distribution, the bigger the sample count, the faster t_critical approach 1.96
          se_approve_disapp = sd(app_disapp)/sqrt(count), # finding standard error
          margin_of_error = t_critical*se_approve_disapp,  
          rating_low = mean_approve_disapprove - margin_of_error, # make our ribbon bands
          rating_high = mean_approve_disapprove+ margin_of_error 
         )

ggplot(approval_polllist, aes(x=week, y=mean_approve_disapprove))+geom_ribbon(aes( ymax = rating_high, ymin=rating_low, fill = subgroup))+ geom_line()+ facet_grid(rows = vars(subgroup)) + labs(x = 'Week in 2022', y ='', title = "Biden's Net Approval Ratings in 2022", subtitle = "Weekly Data, Approve - Disapprove, %")+ylim(-25,-5)+xlim(0,35)

```
For Biden's net approval ratings, we can see similar trends along the 3 sub-groups for the weeks in 2022. 

# Challenge 1: Excess rentals in TfL bike sharing



```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```


## Monthly Changes in in TfL bike rentals

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}

bike1<- bike %>% 
  filter (year > 2016, year < 2023) %>% # to obtain years 2017-2022
  group_by(year, month) %>% # group by year, then month
  mutate(month=match(month,month.abb)) %>% # converting month into numbers (to be able to draw)
  summarize(mon_year_avg = mean(bikes_hired)) # find mean of each year of each month

bike2 <- bike %>% 
  filter(year>2015, year <2020) %>% # second line, draw average 2016-2019
  group_by(month) %>%  #
  mutate(month = match(month,month.abb)) %>%  # converting month into numbers (to be able to draw)
  summarize(mean_months = mean(bikes_hired)) # fund mean of each month throughout the years

#mon_year_avg>mean_months #green
#mean_months>mon_year_avg #red
bike3 <- merge(x=bike1,y=bike2,by ='month') #
ggplot(bike3, aes(x=month))+ #
  geom_line(aes(y=mon_year_avg))+ #plot yearly month average
  geom_line(aes(y=mean_months),colour='blue') + #mean of months
  geom_ribbon(data=bike3,aes(ymax=pmin(mean_months,mon_year_avg),ymin=mon_year_avg),fill="green",alpha=0.5) + #green ribbon when average is more then 2016-2019
  geom_ribbon(data=bike3,aes(ymax=pmin(mean_months,mon_year_avg), ymin=mean_months),fill="red",alpha=0.5)+
  #geom_ribbon(data = bike6, aes(ymax = pmax(per_change, 0), ymin = 0), fill = "green", alpha = 0.5)+
  #geom_ribbon(data = bike6, aes(ymax = 0, ymin = pmin(per_change, 0)), fill = "red", alpha = 0.5)
  facet_wrap(vars(year))+ 
  ylim(10000,45000)+ #limit on y axis
  labs(x="",y="Bike rentals",title="Monthly changes in TfL bike rentals",subtitle="Change from monthly average shown in blue and calculated between 2016-2019") +
  theme_minimal()

```

## Weekly Changes in TfL bike rentals
The second one looks at percentage changes from the expected level of
weekly rentals. The two grey shaded rectangles correspond to Q2 (weeks
14-26) and Q4 (weeks 40-52).

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
#process data to get the percentage difference between expected and actual bike rented per week
bike4<- bike %>% filter (year > 2016, year < 2023) %>% group_by(year, week) %>%   summarize(week_year_avg = mean(bikes_hired))

bike5 <- bike %>% filter(year>2015, year <2020) %>% group_by(week)%>% summarize(mean_weeks = mean(bikes_hired), base_line = 0)

bike6 <- merge(x=bike4,y=bike5,by ='week')
bike6<- bike6 %>% filter() %>% mutate(per_change = (week_year_avg - mean_weeks)/mean_weeks, if_exceed = ifelse(per_change >=0, TRUE, FALSE))

#plot expected and actual bike rental per year 
ggplot(bike6, aes(x=week))+geom_line(aes(group = 1, y=per_change),colour='black')+geom_line(aes(y = base_line )) +facet_wrap(vars(year))+xlim(0, 53)+
#setting different colors for above/below 0
geom_ribbon(data = bike6, aes(ymax = pmax(per_change, 0), ymin = 0), fill = "green", alpha = 0.5)+
geom_ribbon(data = bike6, aes(ymax = 0, ymin = pmin(per_change, 0)), fill = "red", alpha = 0.5)+ scale_y_continuous(labels = scales::percent)+
#add geom_rug
geom_rug(mapping = aes(color = if_exceed), sides = "b")+
#add titles
labs(title = "Weekly changes in Tfl bike rental",
              subtitle = "% change from weekly averages calculated between 2016-2019",
              caption = "Data source: TfL, London Data Store")

```

### Should you use the mean or the median to calculate your expected rentals? Why?

We use the mean to calculate the expected rentals. We used the mean to calculate our expected rentals because the TFL may be more inclined to use the data for profit and revenue analysis. Mean, instead of median, may be more accurate to represent the revenue from bike rental.



# Challenge 2: Share of renewable energy production in the world

The National Bureau of Economic Research (NBER) has a a very interesting
dataset on the adoption of about 200 technologies in more than 150
countries since 1800. This is the[Cross-country Historical Adoption of
Technology (CHAT) dataset](https://www.nber.org/research/data/cross-country-historical-adoption-technology).

The following is a description of the variables

| **variable** | **class** | **description**                |
|--------------|-----------|--------------------------------|
| variable     | character | Variable name                  |
| label        | character | Label for variable             |
| iso3c        | character | Country code                   |
| year         | double    | Year                           |
| group        | character | Group (consumption/production) |
| category     | character | Category                       |
| value        | double    | Value (related to label)       |

```{r,load_technology_data}
technology <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-19/technology.csv')

#get all technologies
labels <- technology %>% 
  distinct(variable, label)

# Get country names using 'countrycode' package
technology <- technology %>% 
  filter(iso3c != "XCD") %>% 
  mutate(iso3c = recode(iso3c, "ROM" = "ROU"),
         country = countrycode(iso3c, origin = "iso3c", destination = "country.name"),
         country = case_when(
           iso3c == "ANT" ~ "Netherlands Antilles",
           iso3c == "CSK" ~ "Czechoslovakia",
           iso3c == "XKX" ~ "Kosovo",
           TRUE           ~ country))

#make smaller dataframe on energy
energy <- technology %>% 
  filter(category == "Energy")

# download CO2 per capita from World Bank using {wbstats} package
# https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1970, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated))

# get a list of countries and their characteristics
# we just want to get the region a country is in and its income level
countries <-  wb_cachelist$countries %>% 
  select(iso3c,region,income_level)

```

This is a very rich data set, not just for energy and CO2 data, but for many other technologies. In our case, we just need to produce a couple of graphs-- at this stage, the emphasis is on data manipulation, rather than making the graphs gorgeous.

First, produce a graph with the countries with the highest and lowest % contribution of renewables in energy production. This is made up of `elec_hydro`, `elec_solar`, `elec_wind`, and `elec_renew_other`. You may want to use the *patchwork* package to assemble the two charts next to each other.
 
```{r}
library(patchwork)

en_new <- energy %>%  #filter energy dataset for year + create new variable for all renewable energy
  filter(year == 2019) %>% 
  group_by(country, variable) %>% 
  summarise(count = sum(value)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = "variable", values_from = "count") %>% 
  mutate(renew_en = elec_hydro + elec_solar + elec_wind + elec_renew_other)

en_new[is.na(en_new)] <- 0 #establish renewable energy usage as a percentage of energy as a whole
en_new <- en_new %>% 
mutate(percent = renew_en/ elecprod*100) %>% 
arrange(desc(percent)) %>% 
filter(renew_en != 0, percent != Inf)
en_new


#create plot 1 for highest % of countries with renewable energy
c2p1 <- ggplot(en_new %>% slice_max(order_by = percent, n = 20), aes(x = percent, y = fct_reorder(country, percent))) + geom_col() + 
labs(title = "Highest and Lowest Percentage of Renewables", x = NULL, y = NULL)
c2p1


#create plot 2 for lowest % of countries with renewable energy
dfsid<- en_new %>%
  slice_min(order_by = percent, n=20)


c2p2 <- ggplot(dfsid, aes(x= percent, y=fct_reorder(country, percent))) + geom_col() + labs(title ='graph2',y = NULL, x = NULL)
c2p2

#combining plots
c2p1 + c2p2

```



Second, you can produce an animation to explore the relationship between CO2 per capita emissions and the deployment of renewables. As the % of energy generated by renewables goes up, do CO2 per capita emissions seem to go down?

To create this animation is actually straight-forward. You manipulate your date, and the create the graph in the normal ggplot way. the only `gganimate` layers you need to add to your graphs are

```
  labs(title = 'Year: {frame_time}', 
       x = '% renewables', 
       y = 'CO2 per cap') +
  transition_time(year) +
  ease_aes('linear')
```
```{r}
library(gifski)
library(png)

new_co2_percap <- merge(co2_percap, countries, by="iso3c") #merge all the data into one  dataset
new_co2_percap <- merge(new_co2_percap, en_new, by="country")

library(gapminder)
library(gganimate)
library(av)
library(tibble)
data <- new_co2_percap[,c(1,2,6,7,9,21)] #selecting columnns from above dataset
library(rmarkdown)

#plot and animate the graph
ggplot(data, aes(x=percent, y=value, color=income_level)) + geom_point() + facet_wrap(~income_level, nrow = 2) + labs(title = 'Year:{round(frame_time, 0)}', x='% of renewable resources', y='CO2 present per cap') + transition_time(date) + ease_aes('linear') +
  theme(legend.position = "none")

```





