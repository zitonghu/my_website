---
title: 'Session 2: Homework 1'
author: "Study Group 7"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(dplyr)
library(psych)
library(ggrepel)
library(patchwork)
```

# Rents in San Francsisco 2000-2018

```{r}
# download directly off tidytuesdaygithub repo

rent <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-05/rent.csv')

```

What are the variable types? Do they all correspond to what they really are? Which variables have most missing values?

This dataset has 2 different types of data sets - character and double. All the variables correspond to the respective data types. The character data type is used for all text based variables like city, country, etc and the double precision data type is used for numeric and date variables. 'descr' has the most missing value, 197542. 'adress' has the second most missing value, of 196888.


```{r skim_data}
skimr::skim(rent)
spec(rent)
unique(rent$city)
```



Make a plot that shows the top 20 cities in terms of % of classifieds between 2000-2018. You need to calculate the number of listings by city, and then convert that number to a %.

```{r top_cities}
rent %>%
  count(city, sort=TRUE) %>% 
  mutate(prop = n/sum(n)) %>% 
  slice_max(n= 20, order_by= prop) %>% 
  mutate(city = fct_reorder(city,prop)) %>% 
  ggplot(aes(x= prop, y= city))+
  geom_col()+
  theme_minimal(base_size = 18)+
  scale_x_continuous(labels = scales::percent)+labs (x = 'prop', y = 'city', caption = "Source: Kate(2018). Bay Area Craiglist Rental Housing Posts, 2000 - 2018")
```

Make a plot that shows the evolution of median prices in San Francisco for 0, 1, 2, and 3 bedrooms listings. 
```{r}
rent_sf<- rent %>%
          filter(city =="san francisco", beds <= 3) %>%
          group_by(year, beds) %>%
          summarise(med_rn = median(price))
 
ggplot(rent_sf, aes(x=year, y=med_rn, color= factor(beds)))+geom_line()+facet_grid(~beds)
```
```{r}
filtered_beds_0_2018 <- rent %>% group_by(beds,year) %>% filter(city =='san francisco',beds == 0, year ==2018) 
head(filtered_beds_0_2018)
```



Finally, make a plot that shows median rental prices for the top 2 cities in the Bay area. Your final graph should look like this
```{r}
top_twelve_data <- rent %>%
  count(city) %>% #to count number of classifieds in each city
  arrange(desc(n)) %>%
  slice(1:12) 
top_twelve_cities <- as.vector(top_twelve_data$city)
top_twelve_cities

rent %>%
  filter(city %in% top_twelve_cities) %>%
  group_by(year, city) %>%
  summarize(median_price = median(price)) %>%
ggplot(aes(year, median_price, color = factor(city))) +
  geom_line() +
  facet_wrap(vars(city)) +
  labs(
  title = "Rental prices for 1-bedroom flats in the Bay Area",
  x = NULL,
  y = NULL,
  caption = "Source: Pennington. Kate (2018). Bay Area Craigslist Rental Housing Posts 2000-2018") +
  theme(legend.position = "None")
```



The data set shows us that San Francisco has the maximum percentage listings, precisely 4x listings more than San Jose, the second highest listings city. This is valid since most listing are in the densely populated area of San Francisco rather than in the suburbs in the vicinity. Moreover, its vicinity to Silicon Valley makes it a hub for residences. 

In San Francisco, the median prices see a major rise across all the types of bedroom (0-3 bedroom flats) across a 20 year period. The drop in rents from 2002 to 2006 was a result of the housing bubble that caused the demand to decrease and rent prices to plummet down. This bubble burst in 2008, post which we see a massive rise in rents (~90% rise in rent from 2010 to 2015). This similar trend was experienced across all the cities in the Bay Area, with a decline from 2002 to 2006, and a multifold rise in the years after that.

# Analysis of movies- IMDB dataset

```{r,load_movies, warning=FALSE, message=FALSE, eval=FALSE}
movies <- read.csv(here::here("data", "movies.csv"))

```


## Use your data import, inspection, and cleaning skills to answer the following:

-   Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries? 
```{r count_NA, warning=FALSE, message=FALSE, eval=FALSE}
count_NA <- sum(is.na(movies)) #counting all the NAs
count_NA
duplicated_count <- sum(duplicated(movies)) #counting all the duplicated
duplicated_count
```


There are no missing values and duplicated values, if we use skim(movies) to check. 

-   Produce a table with the count of movies by genre, ranked in descending order. We can see that Comedy has the most movies listed, followed by Action. Whereas Thriller only has one movie listed.
```{r table}
movies <- read.csv(here::here("data", "movies.csv"))
movie_table<- movies %>%group_by(genre) %>% count(genre) #grouping by genre, then counting.
arrange(movie_table, desc(n))
```

-   Produce a table with the average gross earning and budget (`gross` and `budget`) by genre. Calculate a variable `return_on_budget` which shows how many \$ did a movie make at the box office for each \$ of its budget. Ranked genres by this `return_on_budget` in descending order.

We can see that Musical has the highest return on budget at 28.9, followed by Family which had a return on budget at 10.1. Whereas Thriller has the least return on budget at 0.0008.
```{r}
movies %>% 
  group_by(genre) %>% 
  summarize(return_on_budget = (mean(gross)/mean(budget))) %>%  #creating a new column called return_on _budget
  arrange(desc(return_on_budget))
  
  
```

-   Produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. 

We can see that Steven Spielberg had a highest sum of gross revenue.
```{r}
movies %>% group_by(director) %>% summarize(sum = sum(gross),mean = mean(gross), median = median(gross), std = sd(gross)) %>%  arrange(desc(sum)) %>% slice_head(n = 15)
```

-   Finally, ratings. Produce a table that describes how ratings are distributed by genre. We don't want just the mean, but also, min, max, median, SD and some kind of a histogram or density graph that visually shows how ratings are distributed.
```{r}
 movies %>% 
  group_by(genre) %>% 
  skim()
  
```


## Use `ggplot` to answer the following

-   Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes.

We put "cast_facebook_likes" into the X axis and "gross" to the Y axis. There is a slight positive correlation between 'gross' and 'cast_facebook_like', although there exist many outliers, such as when likes is less than 100. Therefore, the cast is not likely to be a good predictor. However, many data points lies outside the a line of best fit, therefore, we cannot say with good confidence that it is a good predictor.

```{r, gross_on_fblikes}
ggplot(movies, aes(x=cast_facebook_likes, y=gross))+ geom_point() +scale_x_log10() +geom_smooth(method ='lm')
```

-   Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.

Budget appears to be a good predictor of grossing, since the graph shows shows a positive correlation. Although when the budget is less than 1 million dollars, it is not a good predictor.

```{r, gross_on_budget}
  ggplot(movies, aes(x=budget, y=gross))+ geom_point() + geom_smooth(method='lm', se=FALSE)+ scale_x_log10() + labs(title='relationship between budget and gross')
```

-   Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?

Generally, some genre have much more rating than others,for instance, crime, documentary, and mystery. The others, however, have significantly less reviews. For those with high numbers of reviews, such as Action, Adventure, there exist positive correlation between gross and rating which makes IMDB rating a good predictor for those movies.
```{r, gross_on_rating}
  ggplot(movies, aes(x=rating, y=gross))+ geom_point() +geom_smooth(method='lm')+ scale_x_log10() + facet_wrap(~genre) + theme_minimal() + labs(title = 'Relationship between Rating and Gross for each genre')
```

# Returns of financial stocks

```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
```

Based on this dataset, create a table and a bar plot that shows the number of companies per sector, in descending order

```{r companies_per_sector}
companies_per_sector <- nyse %>% 
  group_by(sector) %>% 
  summarise(n_companies = n()) %>% 
  arrange(desc(n_companies))

ggplot(data = companies_per_sector, mapping = aes(x = n_companies, y = fct_reorder(sector,n_companies))) + 
  geom_col() + 
  labs(title = "number of companies per sector", x = "number of companies", y = "sector")
```

Next, let's choose some stocks and their ticker symbols and download some data. You **MUST** choose 6 different stocks from the ones listed below; You should, however, add `SPY` which is the SP500 ETF (Exchange Traded Fund).

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}

myStocks <- c("AAPL","JPM","DIS","DPZ","ANF","TSLA","XOM","SPY" ) %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2022-08-31") %>% group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

Create a table where you summarise monthly returns for each of the stocks and `SPY`; min, max, median, mean, SD.

```{r summarise_monthly_returns}

summarise_monthly_returns <- myStocks_returns_monthly %>% 
  summarize(min = min(monthly_returns), max = max(monthly_returns), median = median(monthly_returns), mean = mean(monthly_returns), sd = sd(monthly_returns))
```

Plot a density plot, using `geom_density()`, for each of the stocks

```{r density_monthly_returns}
ggplot(data = myStocks_returns_monthly,aes (x = monthly_returns)) +
  geom_density() +
  facet_wrap(~symbol) 
```

What can you infer from this plot? Which stock is the riskiest? The least risky?
We can see from the plot that the monthly returns of these stocks follow a normal distribution.

The spread of the density graph indicates the riskiness of the stock. The wider the spread, the higher the risk and vice versa. Visually, the riskiest stock is the BCS because it's more likely to earn negative monthly returns. The least risky stock is SPY.



Finally, make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock

```{r risk_return_plot}
ggplot(summarise_monthly_returns) + 
  aes(x = sd, y = mean, label = symbol) +
  geom_point(color = "red") +
  ggrepel::geom_text_repel()


```

What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?

We can tell that there is a positive correlation between expected monthly returns and riskiness of the stock. This indicates a trade-off between these two financial indicators. However, we do see an outlier here, which is the BCS. BCS exhibits the highest riskiness, yet has the lowest expected return.

# On your own: Spotify

```{r, download_spotify_data}

spotify_songs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')


```

Produce a one-page summary describing this dataset. Here is a non-exhaustive list of questions:

1.  What is the distribution of songs' popularity (`track_popularity`). Does it look like a Normal distribution? 

The track popularity is not normally distributed. As it stands by the graph, the "normal pattern" is lost as we take a look to the lower values (from 0 to 10): these values have an higher probability than the values corresponding to the center of the bell curve. Hence, the distribution is not symmetric, having more probability concentrated in the left side of the curve.
```{r}
ggplot(spotify_songs, aes(x=track_popularity)) + geom_density()

```

2.  There are 12 [audio features](https://developer.spotify.com/documentation/web-api/reference/object-model/#audio-features-object) for each track, including confidence measures like `acousticness`, `liveness`, `speechines`and `instrumentalness`, perceptual measures like `energy`, `loudness`, `danceability` and `valence` (positiveness), and descriptors like `duration`, `tempo`, `key`, and `mode`. How are they distributed? can you roughly guess which of these variables is closer to Normal just by looking at summary statistics? 

By the summary given by the skim function, we can take a look to the histogram of the variables. Generally, the plotted variables have an irregular and skewed distribution. Although, it is possible to see that the tempo and the valence variables actually have a distribution that is similar to a normal one. Comparing those two variables, the closest to a normally distributed one is the tempo.
On the other hand, if we had to look at the summary statistics in order to evaluate the similarity to the normal distribution, we could take a look to the difference between the median and the mean: this value is a good inidicator of the skewness of the distribution, since the furthest the median is from the mean, the more skewed is the distribution. In fact, we can see how the two variables with a normal-like distribution (tempo and valence) also have a mean close to the median.
```{r}
  skim(select(spotify_songs,track_name,acousticness, liveness, speechiness,instrumentalness,energy, loudness, danceability, valence, duration_ms, tempo, key,mode))
```


4.  Is there any relationship between `valence` and `track_popularity`? `danceability` and `track_popularity` ?

As we can see by the two graphs, it is possible to see how the correlation between valence and track popularity as well as the correlation between danceability and track popularity is practically absent. Hence, there is no correlation between those variables.
```{r}
ggplot(spotify_songs, aes(x=valence, y=track_popularity))+ geom_smooth(method= 'lm', se=TRUE) + expand_limits(y=c(0,100))
ggplot(spotify_songs, aes(x=danceability, y=track_popularity)) + geom_smooth(method= 'lm', se=TRUE) + expand_limits(y=c(0,100))
```

5.  `mode` indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. Do songs written on a major scale have higher `danceability` compared to those in minor scale? What about `track_popularity`? 

Given the relative difference (the percentage difference) between the two average danceabilities and the two average popularities, it would be risky to say that changing the mode affects these two values. Since the two averages for both values are really close to each other, it might be more technically correct to say that changing the mode doesn't affect neither the danceability nor the track popularity.
```{r}
modality_and_danceability <- spotify_songs %>% 
  group_by(mode) %>% 
  summarise(average_danceability=mean(danceability))

modality_and_danceability

modality_and_popularity <- spotify_songs %>% 
  group_by(mode) %>% 
  summarise(average_track_popularity=mean(track_popularity))

modality_and_popularity

ggplot(modality_and_danceability, aes(x=mode,y=average_danceability)) + geom_col()
ggplot(modality_and_popularity, aes(x=mode, y=average_track_popularity)) + geom_col()

```


# Challenge 1: Replicating a chart
```{r}
rent_c <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-05/rent.csv')
```

The purpose of this exercise is to reproduce a plot using your `dplyr` and `ggplot2` skills. It builds on exercise 1, the San Francisco rentals data.

You have to create a graph that calculates the cumulative % change for 0-, 1-1, and 2-bed flats between 2000 and 2018 for the top twelve cities in Bay Area, by number of ads that appeared in Craigslist. Your final graph should look like this
```{r}
filtered_top <- rent_c %>% group_by(city) %>% count(city, name = 'count_ads') %>% arrange(desc(count_ads)) %>% head(12) # finding top 12 city by ads

filtered_beds_012 <- rent_c %>% filter(beds <= 2) %>% group_by(year,beds,city) %>% mutate(median_price = median(price)) %>% select(city,beds,year,median_price) %>% distinct(city,.keep_all = TRUE) %>%  arrange(desc(beds),desc(city), year) %>% group_by(city,beds)%>%  mutate(percentage_change = (median_price/median_price[1]-1)) # finding percentage change of each #could be more optimal

merged_df  <- left_join(filtered_top, filtered_beds_012,by='city') # left join the two tables.

ggplot(merged_df, aes(x=year, y = percentage_change,colour=city))+geom_line()+facet_grid(beds~city) + theme(legend.position="none",aspect.ratio=25/12,text = element_text(size=rel(3)),title =element_text(size=10, face='bold'),axis.text.x=element_text(size=7, angle=90),
          strip.text.x = element_text(size=rel(2.4)),
          strip.text.y = element_text(size=rel(4))) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + labs(y = '% change', x = 'Year', title= 'Cumulative % Change in 0, 1 and 2-bed median rentals in the Bay Area', subtitle= '2000-2018')
```


# Challenge 2: 2016 California Contributors plots

As discussed in class, I would like you to reproduce the plot that shows the top ten cities in highest amounts raised in political contributions in California during the 2016 US Presidential election.

```{r challenge2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "challenge2.png"), error = FALSE)
```


```{r, load_CA_data, warnings= FALSE, message=FALSE}
# Make sure you use vroom() as it is significantly faster than read.csv()
CA_contributions <- vroom::vroom(here::here("data","CA_contributors_2016.csv"))

zipcodes<- vroom::vroom(here::here("data","zip_code_database.csv"))

CA_contributions$zip<- as.character(CA_contributions$zip)

CA_contributions_2016 <- left_join(CA_contributions, zipcodes, by="zip")

CA_contri_HC<- CA_contributions_2016 %>% filter(contb_date>"2015-12-31", cand_nm == c("Clinton, Hillary Rodham" )) %>% group_by(cand_nm, primary_city) %>% summarize(fund_per_city=sum(contb_receipt_amt)) %>% arrange(desc(fund_per_city))%>% slice_max(order_by = fund_per_city, n=10) 

CA_contri_DT<- CA_contributions_2016 %>% filter(contb_date>"2015-12-31", cand_nm == c("Trump, Donald J.")) %>% group_by(cand_nm, primary_city) %>% summarize(fund_per_city=sum(contb_receipt_amt)) %>% arrange(desc(fund_per_city))%>% slice_max(order_by = fund_per_city, n=10)

HC_graph<- ggplot(CA_contri_HC, aes(x=fund_per_city, y=reorder(primary_city, fund_per_city)))+geom_col(width = 0.5, fill = "dark blue")+theme_minimal(base_size=10)+
  ggtitle("Clinton, Hillary Rodham")+theme(aspect.ratio = 1/1)

DT_graph<- ggplot(CA_contri_DT, aes(x=fund_per_city, y=reorder(primary_city, fund_per_city)))+geom_col(width = 0.5, fill = "red",position = position_dodge(0.1))+theme_minimal(base_size=10)+ggtitle("Trump, Donald J.")+theme(aspect.ratio = 1/1)
```


```{r, load_CA_data_2, warnings= FALSE, message=FALSE}
HC_graph+DT_graph+plot_annotation(title = "Where did candidates raise most money")
```

# Details

-   Who did you collaborate with: Lejla Kajevic, Francesco Nicolì, Zitong Hu, Rufei Wang, Siddharth Gulati
-   Approximately how much time did you spend on this problem set: 
-   What, if anything, gave you the most trouble: 


